{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e7294-7dd5-4ef6-a1fd-d64e3849deee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import re\n",
    "import xml.etree.ElementTree as ET\n",
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53ad6ad-139f-48ea-8960-18c53ab69048",
   "metadata": {},
   "source": [
    "# Class for parsing and processing the `html` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc789436-a031-4b8e-a0f6-686a118fac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HskHtmlParser(HTMLParser):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dict_content = {}\n",
    "    \n",
    "    def handle_data(self, data):\n",
    "        if \"window.__REACT_DATA = \" in data:\n",
    "            content = data.split(\"window.__REACT_DATA = \")[1][:-2]  # removing the last ';'\n",
    "            self.dict_content = ast.literal_eval(content)\n",
    "\n",
    "            \n",
    "    def create_word_xml_automatic(self, output_file, grammar_indicator={}):\n",
    "        deck = ET.Element('deck', attrib={'name': f'HSK {self.dict_content[\"hskLevel\"]} Word List'})\n",
    "\n",
    "        fields = ET.SubElement(deck, 'fields')\n",
    "        chinese = ET.SubElement(fields, 'chinese', attrib={'name': 'Chinois', 'sides': '10', 'lang': 'zh-CN', 'pinyinMode': 'back'})\n",
    "        text = ET.SubElement(fields, 'text', attrib={'name': 'Traduction', 'sides': '01', 'lang': 'fr-FR'})\n",
    "\n",
    "        cards = ET.SubElement(deck, \"cards\")\n",
    "\n",
    "        for word_entry in self.dict_content[\"words\"]:\n",
    "\n",
    "            hanzi = word_entry[\"hanzi\"]\n",
    "            definition = word_entry[\"def\"]\n",
    "\n",
    "            if hanzi != word_entry[\"hanziRaw\"]:\n",
    "                # Replacing chinese grammar indicators\n",
    "                for key, value in grammar_indicator.items():\n",
    "                    hanzi = hanzi.replace(key, value)\n",
    "\n",
    "            card = ET.SubElement(cards, \"card\")\n",
    "            chinese = ET.SubElement(card, 'chinese', attrib={'name': 'Chinois'})\n",
    "            chinese.text = hanzi\n",
    "            text = ET.SubElement(card, 'text', attrib={'name': 'Traduction'})\n",
    "            text.text = definition\n",
    "\n",
    "        deck_tree = ET.ElementTree(deck)\n",
    "        deck_tree.write(output_file, encoding=\"unicode\")\n",
    "\n",
    "    def create_sentence_xml_automatic(self, output_file):\n",
    "        deck = ET.Element('deck', attrib={'name': f'HSK {self.dict_content[\"hskLevel\"]} Sentence List'})\n",
    "\n",
    "        fields = ET.SubElement(deck, 'fields')\n",
    "        chinese = ET.SubElement(fields, 'chinese', attrib={'name': 'Chinois', 'sides': '10', 'lang': 'zh-CN', 'pinyinMode': 'back'})\n",
    "        text = ET.SubElement(fields, 'text', attrib={'name': 'Traduction', 'sides': '01', 'lang': 'fr-FR'})\n",
    "\n",
    "        cards = ET.SubElement(deck, \"cards\")\n",
    "\n",
    "        for word_entry in self.dict_content[\"localizedSentences\"]:\n",
    "\n",
    "            hanzi = word_entry[\"hanzi\"]\n",
    "            definition = word_entry[\"def\"]\n",
    "\n",
    "            card = ET.SubElement(cards, \"card\")\n",
    "            chinese = ET.SubElement(card, 'chinese', attrib={'name': 'Chinois'})\n",
    "            chinese.text = hanzi\n",
    "            text = ET.SubElement(card, 'text', attrib={'name': 'Traduction'})\n",
    "            text.text = definition\n",
    "\n",
    "        deck_tree = ET.ElementTree(deck)\n",
    "        deck_tree.write(output_file, encoding=\"unicode\")\n",
    "\n",
    "    def create_word_xml(self, output_file, grammar_indicator={}):\n",
    "\n",
    "        deck = ET.Element('deck', attrib={'name': f'HSK {self.dict_content[\"hskLevel\"]} Word List'})\n",
    "\n",
    "        fields = ET.SubElement(deck, 'fields')\n",
    "        front = ET.SubElement(fields, 'text', attrib={'name': 'Front', 'sides': '11', 'lang': 'zh-CN'})  # Visible on both sides\n",
    "        back = ET.SubElement(fields, 'text', attrib={'name': 'Back', 'sides': '01', 'lang': 'zh-CN'})\n",
    "        pinyin = ET.SubElement(fields, 'rich-text', attrib={'name': 'Pinyin', 'sides': '01'})\n",
    "\n",
    "        cards = ET.SubElement(deck, \"cards\")\n",
    "\n",
    "        for word_entry in self.dict_content[\"words\"]:\n",
    "\n",
    "            hanzi = word_entry[\"hanzi\"]\n",
    "            definition = word_entry[\"def\"]\n",
    "            pinyin_tone = word_entry[\"pinyinToneSpace\"]\n",
    "\n",
    "            if hanzi != word_entry[\"hanziRaw\"]:\n",
    "                # Replacing chinese grammar indicators\n",
    "                for key, value in grammar_indicator.items():\n",
    "                    hanzi = hanzi.replace(key, value)\n",
    "\n",
    "            card = ET.SubElement(cards, \"card\")\n",
    "            front = ET.SubElement(card, 'text', attrib={'name': 'Front'})\n",
    "            front.text = hanzi\n",
    "            back = ET.SubElement(card, 'text', attrib={'name': 'Back'})\n",
    "            back.text = definition\n",
    "            pinyin = ET.SubElement(card, 'rich-text', attrib={'name': 'Pinyin'})\n",
    "            italic = ET.SubElement(pinyin, 'i')\n",
    "            italic.text = pinyin_tone\n",
    "\n",
    "            card = ET.SubElement(cards, \"card\")\n",
    "            front = ET.SubElement(card, 'text', attrib={'name': 'Front'})\n",
    "            front.text = definition\n",
    "            back = ET.SubElement(card, 'text', attrib={'name': 'Back'})\n",
    "            back.text = hanzi\n",
    "            pinyin = ET.SubElement(card, 'rich-text', attrib={'name': 'Pinyin'})\n",
    "            italic = ET.SubElement(pinyin, 'i')\n",
    "            italic.text = pinyin_tone\n",
    "\n",
    "        deck_tree = ET.ElementTree(deck)\n",
    "        deck_tree.write(output_file, encoding=\"unicode\")\n",
    "\n",
    "    def create_sentence_xml(self, output_file):\n",
    "        deck = ET.Element('deck', attrib={'name': f'HSK {self.dict_content[\"hskLevel\"]} Sentence List'})\n",
    "\n",
    "        fields = ET.SubElement(deck, 'fields')\n",
    "        front = ET.SubElement(fields, 'text', attrib={'name': 'Front', 'sides': '11', 'lang': 'zh-CN'})  # Visible on both sides\n",
    "        back = ET.SubElement(fields, 'text', attrib={'name': 'Back', 'sides': '01', 'lang': 'zh-CN'})\n",
    "        pinyin = ET.SubElement(fields, 'rich-text', attrib={'name': 'Pinyin', 'sides': '01'})\n",
    "\n",
    "        cards = ET.SubElement(deck, \"cards\")\n",
    "\n",
    "        for word_entry in self.dict_content[\"localizedSentences\"]:\n",
    "\n",
    "            hanzi = word_entry[\"hanzi\"]\n",
    "            definition = word_entry[\"def\"]\n",
    "            pinyin_tone = word_entry[\"pinyinTone\"]\n",
    "\n",
    "            card = ET.SubElement(cards, \"card\")\n",
    "            front = ET.SubElement(card, 'text', attrib={'name': 'Front'})\n",
    "            front.text = hanzi\n",
    "            back = ET.SubElement(card, 'text', attrib={'name': 'Back'})\n",
    "            back.text = definition\n",
    "            pinyin = ET.SubElement(card, 'rich-text', attrib={'name': 'Pinyin'})\n",
    "            italic = ET.SubElement(pinyin, 'i')\n",
    "            italic.text = pinyin_tone\n",
    "\n",
    "        deck_tree = ET.ElementTree(deck)\n",
    "        deck_tree.write(output_file, encoding=\"unicode\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50293c95-df46-4fa7-a32e-2cbe5d237b4d",
   "metadata": {},
   "source": [
    "# Reading the data\n",
    "You need to download html files from [here](https://hsk.academy/fr/hsk-1-vocabulary-list).\n",
    "\n",
    "If the link does not work, copy this link: `https://hsk.academy/fr/hsk-1-vocabulary-list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ea611-46ae-4777-8b1e-0328db054dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"html_files/HSK_1.html\") as f:\n",
    "    html_file = \"\".join(f.readlines())\n",
    "\n",
    "parser = HskHtmlParser()\n",
    "parser.feed(html_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe9b7b5-28cb-4025-ba64-52ac00b99241",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar_indicator = {\n",
    "    \"(助动词)\": \"(verbe auxiliaire)\",\n",
    "    \"(助词)\": \"(particule)\",\n",
    "    \"(动词)\": \"(verbe)\",\n",
    "    \"(叹词)\": \"(interjection)\",\n",
    "    \"(形容词)\": \"(adjectif)\",\n",
    "    \"(介词)\": \"(préposition)\",\n",
    "    \"(副词)\": \"(adverbe)\",\n",
    "    \"(名词)\": \"(nom)\",\n",
    "    \"(量词)\": \"(quantificateur)\"\n",
    "}\n",
    "\n",
    "# Checking if there are other indicators like '(助词)' in\n",
    "# the data that are not already in grammar_indicator\n",
    "parenthesis_regex = re.compile(\"\\(.*?\\)\")\n",
    "list_missing_indicator = []\n",
    "for word_entry in parser.dict_content[\"words\"]:\n",
    "    if word_entry[\"hanzi\"] != word_entry[\"hanziRaw\"]:\n",
    "        parenthesis_words = parenthesis_regex.findall(word_entry[\"hanzi\"])\n",
    "        if parenthesis_words:\n",
    "            list_missing_indicator += [k for k in parenthesis_words if k not in grammar_indicator.keys()]\n",
    "            \n",
    "print(f\"Missing indicators: {list(set(list_missing_indicator))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edabf691-0029-46b0-b50b-b3234fd7556b",
   "metadata": {},
   "source": [
    "# Creating the decks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a435673-06a3-4ce2-96ad-fa6523a18cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic HSK decks with automatic pinyin detection\n",
    "output_word = f\"xml_outputs/automatic_pinyin/HSK_{parser.dict_content['hskLevel']}_word_list.xml\"\n",
    "parser.create_word_xml_automatic(output_word, grammar_indicator=grammar_indicator)\n",
    "\n",
    "output_sentence = f\"xml_outputs/automatic_pinyin/HSK_{parser.dict_content['hskLevel']}_sentence_list.xml\"\n",
    "parser.create_sentence_xml_automatic(output_sentence)\n",
    "\n",
    "\n",
    "# Custom HSK decks with manual pinyin implementation\n",
    "output_word = f\"xml_outputs/HSK_{parser.dict_content['hskLevel']}_word_list.xml\"\n",
    "parser.create_word_xml(output_word, grammar_indicator=grammar_indicator)\n",
    "\n",
    "output_sentence = f\"xml_outputs/HSK_{parser.dict_content['hskLevel']}_sentence_list.xml\"\n",
    "parser.create_sentence_xml(output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e613e5-3d07-4550-82b5-c2b25920a16b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
